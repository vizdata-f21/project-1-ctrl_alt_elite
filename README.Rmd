---
title: Project title
author: Ctrl Alt Elite
output: github_document
---

```{r load-packages, echo = FALSE}
library(tidyverse)
library(maps)
library(sf)
library(rnaturalearth)
library(rvest)
cleanFun <- function(htmlString) {
  return(gsub("<.*?>", "", htmlString))
}
```

```{r load-data-functions, echo = FALSE}
tweets <- read.csv("data/tweets.csv")
us_cities <- read.csv("data/uscities.csv")

# credit to https://github.com/tidyverse/dplyr/issues/2278

```

## Introduction

(1-2 paragraphs)
Brief introduction to the dataset.
You may repeat some of the information about the dataset provided in the introduction to the dataset on the TidyTuesday repository, paraphrasing on your own terms.
Imagine that your project is a standalone document and the grader has no prior knowledge of the dataset.

## Question 1 <- Update title to relate to the question you’re answering

### Introduction

(1-2 paragraphs)
Introduction to the question and what parts of the dataset are necessary to answer the question. Also discuss why you’re interested in this question.

### Approach

(1-2 paragraphs)
Describe what types of plots you are going to make to address your question.
For each plot, provide a clear explanation as to why this plot (e.g. boxplot, barplot, histogram, etc.) is best for providing the information you are asking about. 
The two plots should be of different types, and at least one of the two plots needs to use either color mapping or facets.

### Analysis

We will first create a bar plot that maps `location` on the y-axis and the 
number of tweets from that location on the x-axis. This will require creating a 
new variable `location_tweet_count`. As a user in one location can tweet 
multiple times, we will consider all their tweets cumulatively to look at 
total geographical engagement of users on this plot. This is due to the fact 
that we consider multiple tweets by a user to be a sign of engagement, and thus 
want to understand engagement on the whole, versus on a per user basis. We will 
also need to mutate a new variable `location_state`, based off of `location`, 
where location names are modified to be a state in the United States, or 
categorized as an international country. If there happen to be many 
international locations in the data set from different counties, we will 
consider having two bar plots - one for U.S. states, and one for other 
countries, so that the visualization does not become too overcrowded. We could 
also only display the top 10 or so locations on the bar plot, if need be for 
simplification purposes (we do not want too much overwhelming info). We will 
also need to look through the location data to remove any observations which do
not represent actual names of geographical locations. By looking at this data 
and organizing it on a bar plot, we will be able to compare engagement in the 
TidyTuesday challenge geographically, to see which locations produced the most 
number of tweets. The advantage of using a bar plot is that each location is 
very distinguishable from the next, and the data can be ordered such that it's 
obvious visually which locations have many tweets, versus very few tweets coming 
from them. This will also inform our next visualization, as we will know which 
areas of the global map to emphasize, if need be.

```{r question-one-plot-one-wrangling}
tweets %>%
  group_by(location) %>%
  summarize(location_tweet_count = n())

# Data frame of cities of the world and corresponding countries
world_cities <- world.cities %>% 
  as_tibble() %>% 
  rename(c("city" = "name"))

# Data frame of cities in US and corresponding states
us_cities <- us_cities %>% 
  select(city, state_id, state_name)

# create new variable city that is the location pre ,
tweets_locations <- tweets %>% 
  filter(!str_detect(location, "@"), !str_detect(location, ":")) %>% 
  mutate(location_pre_comma = gsub(",.*","", location)) %>%
  mutate(location_pre_comma = case_when(
    location_pre_comma == "北京" ~ "Beijing",
    location_pre_comma == "God's earth" ~ "NA",
    location_pre_comma == "The City College of New York" ~ "New York",
    location_pre_comma == "World" ~ "NA",
    location_pre_comma == "he/they" ~ "NA",
    location_pre_comma == "At the home office" ~ "NA",
    location_pre_comma == "Deutschland" ~ "Germany",
    location_pre_comma == "Distrito Federal" ~ "Mexico City",
    location_pre_comma == "down in dey wid em" ~ "NA",
    location_pre_comma == "Forde-Obama Hall" ~ "NA",
    location_pre_comma == "France & UK" ~ "NA",
    location_pre_comma == "Lil’ Rudyshire" ~ "NA",
    location_pre_comma == "MIT" ~ "Boston",
    location_pre_comma == "New Yorker" ~ "New York",
    location_pre_comma == "OAK / NYC / ATL / The World" ~ "NA",
    location_pre_comma == "SP" ~ "NA",
    location_pre_comma == "Toronto || Ottawa" ~ "NA",
    location_pre_comma == "Tx" ~ "Texas",
    location_pre_comma == "UK" ~ "United Kingdom",
    location_pre_comma == "USA" ~ "United States",
    location_pre_comma == "Worldwide" ~ "NA",
    TRUE ~ location_pre_comma
  ))
  
tweets_locations <- tweets_locations %>%
  mutate(plot_state = case_when(
    location_pre_comma == "Nashville" ~ "Tennessee",
    location_pre_comma =="Merced" ~ "California",
    location_pre_comma == "Vienna" ~ "Austria",
    location_pre_comma == "Madison" ~ "Wisconsin",
    location_pre_comma == "Minneapolis" ~ "Minnesota",
    location_pre_comma == "Philadelphia" ~ "Pennsylvania",
    location_pre_comma == "Boston" ~ "Massachusetts",
    location_pre_comma == "London" ~ "United Kingdom",
    location_pre_comma == "Edinburgh" ~ "United Kingdom",
    location_pre_comma == "Amherst" ~ "Massachusetts",
    location_pre_comma == "Buffalo" ~ "New York",
    location_pre_comma == "Cambridge" ~ "Massachusetts",
    location_pre_comma == "San Diego" ~ "California",
    TRUE ~ as.character(location_pre_comma)
  ))

grouped <- tweets_locations %>%
  count(plot_state) %>%
  arrange(desc(n)) %>%
  filter(plot_state != "NA") %>%
  head(10) %>%
  mutate(internat = case_when(
    plot_state == "United Kingdom" ~ "International",
    plot_state == "Austria" ~ "International",
    TRUE ~ "Domestic"))

grouped %>%
  ggplot(aes(y = fct_reorder(plot_state, n), x = n, fill = internat)) +
  geom_col()
```

```{r}
tweets %>%
  count(location) %>%
  arrange(desc(n)) %>% 
  head(15)
```

Idea for visualization 1 is to only do data analysis for international places.


(2-3 code blocks, 2 figures, text/code comments as needed)
In this section, provide the code that generates your plots. Use scale functions to provide nice axis labels and guides.
You are welcome to use theme functions to customize the appearance of your plot, but you are not required to do so.
All plots must be made with ggplot2. Do not use base R or lattice plotting functions.

To next explore this inquiry, we will use the `lat`, `long`, and `retweet_count` 
variables. To display the geographical distribution of the data, we will use an 
external map of the world, on which we will layer a scatter plot of the location 
of each tweet (where it was tweeted from). We will use the latitude and 
longitude variables to accomplish this, along with the world map coordinates 
that come from the map_data() function in tidyverse. Then, we will display the 
count of retweets for each tweet by mapping the `retweet_count` variable to 
color, size, or another scatterplot aesthetic, such that all of the information 
is very clearly displayed on the world map. Since it will be important to 
consider the population differences of the locations on the map when analyzing 
the occurrence of tweets, we will create a separate heatmap of the world 
(we will use external data of the 2021 population of the world's countries), to 
be displayed next to this map, to help readers take population discrepancies 
into account. Thus, this visualization will most likely employ color mapping. 

```{r question-one-plot-two}
# Loading posible map to use
world_map <- map_data("world")

us_map <- map_data("state")

canada_map <- map_data("world","canada")


 ggplot() +
   geom_polygon(data = world_map, aes(x = long, y = lat, group = group), fill = "gray", color = "black") +
   geom_point(data = tweets, aes(x = long, y = lat, size = retweet_count), color = "red") +
   theme_minimal() +
   coord_fixed(1.3)

  #coord_quickmap()



 
 tweets <- tweets%>%
  mutate(continent=case_when(long < -46 & (lat>20 & lat < 46) ~ "North America", 
                             (long > -100  & long<50) &(lat>-100 & lat <20) ~ "South America", 
                             (long > -50  & long<60) &(lat>-50 & lat <50) ~ "Africa", 
                             
                            (long > 25) &(lat>-50 & lat <50) ~ "Europe", 
                            
                          (long > 50  & long<150) &(lat>-12.5) ~ "Asia", 
                          
                        (long > 100) &(lat<-12.5) ~ "Oceania"))

 
northeast <- tweets %>%
  filter(long <= -70,
         long >= -90,
         lat <= 46,
         lat >= 20)

  ggplot() +
   geom_polygon(data=canada_map, aes(x = long, y = lat, group = group), fill = "gray", color = "black") +
  geom_polygon(data = us_map, aes(x = long, y = lat, group = group), fill = "gray", color = "black") +
    geom_point(data = northeast, aes(x = long, y = lat, size = retweet_count), color = "red") +
   theme_minimal() +
   coord_map(xlim = c(-80, -65),
             ylim = c(36, 46))
  
```


```{r North_America}

tweets_NA<-tweets%>%
  filter(continent=="North America")



 #just mapping us data, need to get rid of canada
 ggplot() +
   geom_polygon(data = us_map, aes(x = long, y = lat, group = group), fill = "white", color = "black") +
   geom_point(data = tweets_NA, aes(x = long, y = lat, size = like_count)) +
   theme_minimal() +
   coord_quickmap()

#north_am <- worldmap[worldmap$continent == 'North America',]
#us <- worldmap[worldmap$name == 'United States',]

# New US map option
# ggplot() + 
#   geom_sf(data = us) + 
#   theme_bw() +
#   geom_point(data = tweets_NA, aes(x = long, y = lat, size = retweet_count), color = "red") 

```
 Need to remove 3 canadian values from map. 



### Discussion

(1-3 paragraphs)
In the Discussion section, interpret the results of your analysis. Identify any trends revealed (or not revealed) by the plots.
Speculate about why the data looks the way it does.

## Question 2 <- Update title to relate to the question you’re answering

The second question we want to answer is: 

*How does the time of day affect user construction of #DuBois 
challenge tweets and how the audience reacts to those tweets?*

First, we'll compute the 
length of each tweet's content, operationalized by the number of characters, 
including spaces and emojis, in the tweet, and assign it to a numeric variable 
`tweet_length`.

```{r}
library(lubridate)

# tweets <- tweets %>%
#   filter(!is.na(datetime))%>%
#   mutate(time = (as.numeric(str_sub(datetime, start=12, end=13))),
#         date = ymd(str_sub(datetime, start = 1, end = 11)),
#         content = str_replace_all(as.character(content), "&amp", ""),
#         content = str_replace_all(as.character(content), "\\^@", ""),
#         tweet_length = nchar(as.character(content)))

#We intend to plot number of @'s in a tweet


# tweets %>%
#   arrange(desc(tweet_length)) %>%
#   select(content, tweet_length) %>%
#   head(5)
# 
# ggplot(tweets, aes(x=date, y = tweet_length)) + 
#   geom_col()




```
```{r}
tweets %>%
  filter(verified == TRUE) %>%
  nrow()
```


Thus, for a second visualization, we will use the variables `followers`, 
`verified` and `like_count`. We will map `followers` to the x axis, and 
`like_count` to the y axis, and then display the data for each twitter user 
using geom_point. Since some users may have tweeted multiple times, we will 
calculate `average_like_count` per tweet for each user, and use that number in 
place of the actual `like_count` variable. This means for users who only tweeted 
once, we will use the like count for that tweet, while for users who tweeted 
multiple times, we will use the average like count across all of their challenge 
tweets. We chose to focus on individual users since in the previous 
visualization we were looking into cumulative tweets, on a non per-user basis. 
This way, for this question, we will be able to gauge Twitter engagement in the 
Dubois challenge on a per user basis, marking a contrast between our questions.
Then, we will use color mapping to map the verification status of the tweet 
author to the colors of the points, such that viewers can easily distinguish 
between the two groups (if the groups are not distinguishable, another option is 
to facet by verification status - we will make a choice about this after seeing 
how the data looks). Further, we'll display a geom_smooth regression line for
verified and unverified users (if these lines become visually distracting, we 
will reconsider using them). With these lines, we'll be able to examine the 
interaction between verification and followers and its effect on the number of 
likes a tweet can generate.

```{r}
# tweets <- tweets %>%
#   group_by(username) %>%
#   mutate(average_like_count = mean(like_count))
# 
# 
# tweets_time<- tweets%>%
#   mutate(time_range=case_when(time<=7~"Time 1", 
#                               time>7 & time <=15~ "Time 2", 
#                               time >15~"Time 3"))
# ggplot(tweets_time, aes(followers, average_like_count))+
#   geom_point()+
#   facet_grid(~time_range)+
#   scale_x_continuous(limits=c(0, 10000))+
#   scale_y_continuous(limits=c(0, 200))
#   
  
```



### Introduction

(1-2 paragraphs)
Introduction to the question and what parts of the dataset are necessary to answer the question. Also discuss why you’re interested in this question.

### Approach

(1-2 paragraphs)
Describe what types of plots you are going to make to address your question.
For each plot, provide a clear explanation as to why this plot (e.g. boxplot, barplot, histogram, etc.) is best for providing the information you are asking about. 
The two plots should be of different types, and at least one of the two plots needs to use either color mapping or facets.

### Analysis

(2-3 code blocks, 2 figures, text/code comments as needed)
In this section, provide the code that generates your plots. Use scale functions to provide nice axis labels and guides.
You are welcome to use theme functions to customize the appearance of your plot, but you are not required to do so.
All plots must be made with ggplot2. Do not use base R or lattice plotting functions.

### Discussion

(1-3 paragraphs)
In the Discussion section, interpret the results of your analysis. Identify any trends revealed (or not revealed) by the plots.
Speculate about why the data looks the way it does.

## Presentation

Our presentation can be found [here](presentation/presentation.html).

## Data 

Include a citation for your data here. 
See http://libraryguides.vu.edu.au/c.php?g=386501&p=4347840 for guidance on proper citation for datasets. 
If you got your data off the web, make sure to note the retrieval date.

## References

List any references here. You should, at a minimum, list your data source.
